<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="深度学习,pytorch,"><meta name="description" content="​"><meta property="og:type" content="article"><meta property="og:title" content="深度学习4个实验"><meta property="og:url" content="http://cfhyxxj.top/category/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%AE%9E%E9%AA%8C/index.html"><meta property="og:site_name" content="春风化雨的博客"><meta property="og:description" content="​"><meta property="article:published_time" content="2022-04-24T11:00:00.000Z"><meta property="article:modified_time" content="2022-05-01T13:34:13.056Z"><meta property="article:author" content="春风化雨"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="pytorch"><meta name="twitter:card" content="summary"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://cfhyxxj.top/category/深度学习4个实验/"><title>深度学习4个实验 | 春风化雨的博客</title><meta name="generator" content="Hexo 4.2.0"></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="bg_content"><canvas id="canvas"></canvas></div><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">春风化雨的博客</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-question-circle"></i><br>归档</a></li><li class="menu-item menu-item-guestbook"><a href="/guestbook/" rel="section"><i class="menu-item-icon fa fa-fw fa-comment"></i><br>留言</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://cfhyxxj.top/category/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%AE%9E%E9%AA%8C/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="春风化雨"><meta itemprop="description" content=""><meta itemprop="image" content="/images/zuozhu.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="春风化雨的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">深度学习4个实验</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-04-24T19:00:00+08:00">2022-04-24</time> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2022-05-01T21:34:13+08:00">2022-05-01</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E4%B8%93%E4%B8%9A%E7%9B%B8%E5%85%B3/" itemprop="url" rel="index"><span itemprop="name">专业相关</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E4%B8%93%E4%B8%9A%E7%9B%B8%E5%85%B3/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/category/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%AE%9E%E9%AA%8C/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/category/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%AE%9E%E9%AA%8C/" itemprop="commentCount"></span></a></span> <span id="/category/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04%E4%B8%AA%E5%AE%9E%E9%AA%8C/" class="leancloud_visitors" data-flag-title="深度学习4个实验"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">当前热度&#58;</span><span class="leancloud-visitors-count"></span> <span>℃</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">4.8k 字</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">25 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>​<a id="more"></a></p><h3 id="mnist手写数字识别"><a href="#mnist手写数字识别" class="headerlink" title="mnist手写数字识别"></a>mnist手写数字识别</h3><h4 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential, ReLU, Dropout, Softmax</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model1</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">3136</span>, <span class="number">128</span>),</span><br><span class="line">            Linear(<span class="number">128</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#LeNet5</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model2</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">400</span>, <span class="number">120</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            Linear(<span class="number">84</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#dropout</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model3</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>, padding=<span class="number">2</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            Dropout(<span class="number">0.8</span>),</span><br><span class="line"></span><br><span class="line">            Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Dropout(<span class="number">0.8</span>),</span><br><span class="line"></span><br><span class="line">            Flatten(),</span><br><span class="line">            Dropout(<span class="number">0.8</span>),</span><br><span class="line"></span><br><span class="line">            Linear(<span class="number">400</span>, <span class="number">120</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line"></span><br><span class="line">            Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            Dropout(<span class="number">0.5</span>),</span><br><span class="line"></span><br><span class="line">            Linear(<span class="number">84</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model4</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            Dropout(<span class="number">0.8</span>),</span><br><span class="line"></span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            Dropout(<span class="number">0.8</span>),</span><br><span class="line"></span><br><span class="line">            Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Dropout(<span class="number">0.8</span>),</span><br><span class="line"></span><br><span class="line">            Linear(<span class="number">1152</span>, <span class="number">625</span>),</span><br><span class="line">            ReLU(),</span><br><span class="line">            Dropout(<span class="number">0.5</span>),</span><br><span class="line"></span><br><span class="line">            Linear(<span class="number">625</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    model = Model4()</span><br><span class="line">    input = torch.ones(<span class="number">64</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    output = model(input)</span><br><span class="line">    print(output.shape)</span><br></pre></td></tr></tbody></table></figure><h4 id="train-py"><a href="#train-py" class="headerlink" title="train.py"></a>train.py</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment">#网络模型、数据(输入、标注)、损失函数 可以用.cuda()----&gt;使用gpu加速</span></span><br><span class="line"><span class="comment">#训练集每batch_size保存(batch_size的train_loss)，并输出到tensorboard</span></span><br><span class="line"><span class="comment">#测试集每一个epoch(也就是把测试集都过一遍)保存(总体的test_loss)、(total_test_accuracy)，并输出到tensorboard</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义训练的设备</span></span><br><span class="line"><span class="comment">#device = torch.device("cpu")</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line"></span><br><span class="line">transfrom = transforms.Compose([transforms.ToTensor(),</span><br><span class="line">                                transforms.Normalize([<span class="number">0.5</span>, ], [<span class="number">0.5</span>, ])])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_set = torchvision.datasets.MNIST(<span class="string">"../mnist_dataset"</span>, train=<span class="literal">True</span>, transform=transfrom,</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.MNIST(<span class="string">"../mnist_dataset"</span>, train=<span class="literal">False</span>, transform=transfrom,</span><br><span class="line">                                        download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 长度</span></span><br><span class="line">train_len = len(train_set)</span><br><span class="line">test_len = len(test_set)</span><br><span class="line">print(<span class="string">"训练集的长度：{}"</span>.format(train_len))</span><br><span class="line">print(<span class="string">"测试集的长度：{}"</span>.format(test_len))</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用DataLoader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_set, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_set, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#搭建模型</span></span><br><span class="line">model = Model4()</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #加载已训练的模型</span></span><br><span class="line"><span class="comment"># model = torch.load("mnist_model4_0.9942.pth")      #to-do</span></span><br><span class="line"><span class="comment"># #print(model)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment">#优化器</span></span><br><span class="line"><span class="comment">#learning_rate = 0.01</span></span><br><span class="line"><span class="comment">#1e-2 = 1*(10)^(-2)</span></span><br><span class="line">learning_rate = <span class="number">1e-4</span>  <span class="comment">#to-do</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置训练网络的一些参数</span></span><br><span class="line"><span class="comment">#记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment">#记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"><span class="comment">#记录训练的轮数</span></span><br><span class="line">epoch = <span class="number">300</span>  <span class="comment"># to-do</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">"../log_train"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练集每batch_size保存(batch_size的train_loss)，并输出到tensorboard</span></span><br><span class="line"><span class="comment">#测试集每一个epoch(也就是把测试集都过一遍)保存(总体的test_loss)、(total_test_accuracy)，并输出到tensorboard</span></span><br><span class="line"><span class="comment">#训练步骤开始</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">    print(<span class="string">"-------------第{}轮训练开始-------------"</span>.format(i+<span class="number">1</span>))</span><br><span class="line">    total_train_true_num = <span class="number">0</span></span><br><span class="line">    <span class="comment">#训练步骤开始</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        <span class="comment">#print(imgs.shape)</span></span><br><span class="line">        targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">        outputs = model(imgs)</span><br><span class="line">        <span class="comment">#print(outputs.shape)</span></span><br><span class="line"></span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">        train_true_num = (outputs.argmax(<span class="number">1</span>) == targets).sum()</span><br><span class="line">        total_train_true_num += train_true_num</span><br><span class="line"></span><br><span class="line">        <span class="comment">#优化器优化模型</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            print(end_time - start_time)</span><br><span class="line">            print(<span class="string">"训练次数：{}, loss：{}"</span>.format(total_train_step, loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">"train_loss"</span>, loss.item(), total_train_step)</span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    total_train_accuracy = total_train_true_num / train_len</span><br><span class="line"></span><br><span class="line">    <span class="comment">#测试步骤开始</span></span><br><span class="line">    model.eval()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_test_true_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            imgs = imgs.to(device)</span><br><span class="line">            targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line">            total_test_loss += loss.item()</span><br><span class="line">            test_true_num = (outputs.argmax(<span class="number">1</span>)==targets).sum()</span><br><span class="line">            total_test_true_num += test_true_num</span><br><span class="line"></span><br><span class="line">    total_test_accuracy = total_test_true_num / test_len</span><br><span class="line">    print(<span class="string">"整个训练集上的正确率：{}，整个测试集上的loss：{}，整个测试集上的正确率：{}"</span>.format(total_train_accuracy, total_test_loss, total_test_accuracy))</span><br><span class="line">    writer.add_scalar(<span class="string">"test_loss"</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">"test_accuracy"</span>, total_test_accuracy, total_test_step)</span><br><span class="line">    total_test_step += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span>(total_train_accuracy == <span class="number">1.0</span> <span class="keyword">and</span> total_test_accuracy &gt; <span class="number">0.9940</span>):</span><br><span class="line">        torch.save(model, <span class="string">"mnist_model4_{}.pth"</span>.format(i))</span><br><span class="line">    <span class="comment">#torch.save(tudui.state_dict(), "tudui_{}.pth".format(i))</span></span><br><span class="line">    print(<span class="string">"模型已保存"</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></tbody></table></figure><h3 id="猫狗分类"><a href="#猫狗分类" class="headerlink" title="猫狗分类"></a>猫狗分类</h3><h4 id="get-data-py"><a href="#get-data-py" class="headerlink" title="get_data.py"></a>get_data.py</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 首先将图片分为猫狗图片</span></span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#数据集目录</span></span><br><span class="line">path = <span class="string">"train and test"</span></span><br><span class="line"><span class="comment">#训练集目录</span></span><br><span class="line">train_path = os.path.join(path, <span class="string">"train"</span>)</span><br><span class="line"><span class="comment">#测试集目录</span></span><br><span class="line">test_path = os.path.join(path, <span class="string">"test"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将某类图片移动到该类的文件夹下</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_to_file</span><span class="params">(temp_path)</span>:</span></span><br><span class="line">    imgs_path = os.path.join(path, <span class="string">"*.jpg"</span>)</span><br><span class="line">    print(<span class="string">"共：{}张图片"</span>.format(len(glob.glob(imgs_path))))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#如果没有dog类和cat类文件夹，则新建</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(temp_path,<span class="string">"dog"</span>)):</span><br><span class="line">            os.makedirs(os.path.join(temp_path,<span class="string">"dog"</span>))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(temp_path,<span class="string">"cat"</span>)):</span><br><span class="line">            os.makedirs(os.path.join(temp_path,<span class="string">"cat"</span>))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"=========开始移动图片============"</span>)</span><br><span class="line">    <span class="comment">#通过glob遍历到所有的.jpg文件</span></span><br><span class="line">    <span class="keyword">for</span> img_path <span class="keyword">in</span> glob.glob(imgs_path):</span><br><span class="line">        <span class="comment">#print(img_path)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#使用/划分 得到的是一个列表</span></span><br><span class="line">        img = img_path.strip(<span class="string">"\n"</span>).replace(<span class="string">"\\"</span>,<span class="string">"/"</span>).split(<span class="string">"/"</span>)</span><br><span class="line">        print(img)</span><br><span class="line">        <span class="comment">#将图片移动到指定的文件夹中</span></span><br><span class="line">        <span class="keyword">if</span> img[<span class="number">-1</span>].split(<span class="string">"."</span>)[<span class="number">0</span>] == <span class="string">"cat"</span>:</span><br><span class="line">            shutil.move(img_path, os.path.join(temp_path, <span class="string">"cat"</span>))</span><br><span class="line">        <span class="keyword">if</span> img[<span class="number">-1</span>].split(<span class="string">"."</span>)[<span class="number">0</span>] == <span class="string">"dog"</span>:</span><br><span class="line">            shutil.move(img_path, os.path.join(temp_path, <span class="string">"dog"</span>))</span><br><span class="line">    print(<span class="string">"=========移动图片完成============"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#然后从cat中和dog中分别抽取1250张，共2500张图片作为测试集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_train_test</span><span class="params">(fileDir,tarDir)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(tarDir):</span><br><span class="line">            os.makedirs(tarDir)</span><br><span class="line"></span><br><span class="line">        pathDir = os.listdir(fileDir)    <span class="comment">#取图片的原始路径</span></span><br><span class="line">        filenumber=len(pathDir)</span><br><span class="line">        rate=<span class="number">0.1</span>    <span class="comment">#自定义抽取图片的比例，比方说12500张抽1250张，那就是0.1</span></span><br><span class="line">        picknumber=int(filenumber*rate) <span class="comment">#按照rate比例从文件夹中取一定数量图片</span></span><br><span class="line">        sample = random.sample(pathDir, picknumber)  <span class="comment">#随机选取picknumber数量的样本图片</span></span><br><span class="line">        <span class="comment">#print(sample)</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"=========开始移动图片============"</span>)</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> sample:</span><br><span class="line">                shutil.move(os.path.join(fileDir, name), tarDir)</span><br><span class="line">        print(<span class="string">"=========移动图片完成============"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># img_to_file(train_path) #划分cat和dog</span></span><br><span class="line"><span class="comment"># print("训练集猫共：{}张图片".format(len(glob.glob(os.path.join(train_path,"cat/*.jpg")))))</span></span><br><span class="line"><span class="comment"># print("训练集狗共：{}张图片".format(len(glob.glob(os.path.join(train_path,"dog/*.jpg")))))</span></span><br><span class="line"></span><br><span class="line">split_train_test(os.path.join(train_path,<span class="string">"cat"</span>), os.path.join(test_path,<span class="string">"cat"</span>))</span><br><span class="line">split_train_test(os.path.join(train_path,<span class="string">"dog"</span>), os.path.join(test_path,<span class="string">"dog"</span>))</span><br><span class="line">print(<span class="string">"测试集猫共：{}张图片"</span>.format(len(glob.glob(os.path.join(test_path,<span class="string">"cat/*.jpg"</span>)))))    <span class="comment">#从train中拿出1250张cat</span></span><br><span class="line">print(<span class="string">"测试集狗共：{}张图片"</span>.format(len(glob.glob(os.path.join(test_path,<span class="string">"dog/*.jpg"</span>)))))    <span class="comment">#从train中拿出1250张dog</span></span><br></pre></td></tr></tbody></table></figure><h4 id="train-py-1"><a href="#train-py-1" class="headerlink" title="train.py"></a>train.py</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment">#网络模型、数据(输入、标注)、损失函数 可以用.cuda()----&gt;使用gpu加速</span></span><br><span class="line"><span class="comment">#训练集每batch_size保存(batch_size的train_loss)，并输出到tensorboard</span></span><br><span class="line"><span class="comment">#测试集每一个epoch(也就是把测试集都过一遍)保存(总体的test_loss)、(total_test_accuracy)，并输出到tensorboard</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">torch.cuda.manual_seed_all(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"><span class="comment">#torch.backends.cudnn.benchmark = False</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_path = <span class="string">"train and test/train"</span></span><br><span class="line">test_path = <span class="string">"train and test/test"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义训练的设备</span></span><br><span class="line"><span class="comment">#device = torch.device("cpu")</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([transforms.Resize(<span class="number">224</span>),</span><br><span class="line">                                transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                transforms.ToTensor(),</span><br><span class="line">                                transforms.Normalize([<span class="number">0.5</span>, ], [<span class="number">0.5</span>, ])])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用torchvision.datasets.ImageFolder读取数据集指定train和test文件</span></span><br><span class="line">train_data = torchvision.datasets.ImageFolder(train_path, transform=transform)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.ImageFolder(test_path, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(train_data.classes)  # 根据分的文件夹的名字来确定的类别</span></span><br><span class="line"><span class="comment"># print(train_data.class_to_idx)  # 按顺序为这些类别定义索引为0,1...</span></span><br><span class="line"><span class="comment"># print(train_data.imgs)  # 返回从所有文件夹中得到的图片的路径以及其类别</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(test_data.classes)  # 根据分的文件夹的名字来确定的类别</span></span><br><span class="line"><span class="comment"># print(test_data.class_to_idx)  # 按顺序为这些类别定义索引为0,1...</span></span><br><span class="line"><span class="comment"># print(test_data.imgs)  # 返回从所有文件夹中得到的图片的路径以及其类别</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 长度</span></span><br><span class="line">train_len = len(train_data)</span><br><span class="line">test_len = len(test_data)</span><br><span class="line">print(<span class="string">"训练集的长度：{}"</span>.format(train_len))</span><br><span class="line">print(<span class="string">"测试集的长度：{}"</span>.format(test_len))</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用DataLoader加载数据集</span></span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #搭建模型</span></span><br><span class="line"><span class="comment"># model =torchvision.models.resnet18(pretrained=True)</span></span><br><span class="line"><span class="comment"># #model.add_module("add_linear", nn.Linear(1000, 2))</span></span><br><span class="line"><span class="comment"># model.fc = nn.Linear(512, 2)</span></span><br><span class="line"><span class="comment"># model = model.to(device)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#加载已训练的模型</span></span><br><span class="line">model = torch.load(<span class="string">"resnet18_catdog_0.9896.pth"</span>)      <span class="comment">#to-do</span></span><br><span class="line"><span class="comment">#print(model)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment">#优化器</span></span><br><span class="line"><span class="comment">#learning_rate = 0.01</span></span><br><span class="line"><span class="comment">#1e-2 = 1*(10)^(-2)</span></span><br><span class="line">learning_rate = <span class="number">1e-6</span>  <span class="comment">#to-do</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置训练网络的一些参数</span></span><br><span class="line"><span class="comment">#记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment">#记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"><span class="comment">#记录训练的轮数</span></span><br><span class="line">epoch = <span class="number">300</span>  <span class="comment"># to-do</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">"log_train"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练集每batch_size保存(batch_size的train_loss)，并输出到tensorboard</span></span><br><span class="line"><span class="comment">#测试集每一个epoch(也就是把测试集都过一遍)保存(总体的test_loss)、(total_test_accuracy)，并输出到tensorboard</span></span><br><span class="line"><span class="comment">#训练步骤开始</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">    print(<span class="string">"-------------第{}轮训练开始-------------"</span>.format(i+<span class="number">1</span>))</span><br><span class="line">    total_train_true_num = <span class="number">0</span></span><br><span class="line">    <span class="comment">#训练步骤开始</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        <span class="comment">#print(imgs.shape)</span></span><br><span class="line">        targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">        outputs = model(imgs)</span><br><span class="line">        <span class="comment">#print(outputs.shape)</span></span><br><span class="line"></span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">        train_true_num = (outputs.argmax(<span class="number">1</span>) == targets).sum()</span><br><span class="line">        total_train_true_num += train_true_num</span><br><span class="line"></span><br><span class="line">        <span class="comment">#优化器优化模型</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            print(end_time - start_time)</span><br><span class="line">            print(<span class="string">"训练次数：{}, loss：{}"</span>.format(total_train_step, loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">"train_loss"</span>, loss.item(), total_train_step)</span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    total_train_accuracy = total_train_true_num / train_len</span><br><span class="line"></span><br><span class="line">    <span class="comment">#测试步骤开始</span></span><br><span class="line">    model.eval()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_test_true_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            imgs = imgs.to(device)</span><br><span class="line">            targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line">            total_test_loss += loss.item()</span><br><span class="line">            test_true_num = (outputs.argmax(<span class="number">1</span>)==targets).sum()</span><br><span class="line">            total_test_true_num += test_true_num</span><br><span class="line"></span><br><span class="line">    total_test_accuracy = total_test_true_num / test_len</span><br><span class="line">    print(<span class="string">"整个训练集上的正确率：{}，整个测试集上的loss：{}，整个测试集上的正确率：{}"</span>.format(total_train_accuracy, total_test_loss, total_test_accuracy))</span><br><span class="line">    writer.add_scalar(<span class="string">"test_loss"</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">"test_accuracy"</span>, total_test_accuracy, total_test_step)</span><br><span class="line">    total_test_step += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span>(total_train_accuracy &gt; <span class="number">0.9999</span> <span class="keyword">and</span> total_test_accuracy &gt; <span class="number">0.9896</span>):</span><br><span class="line">        torch.save(model, <span class="string">"resnet18_catdog_{}.pth"</span>.format(i))</span><br><span class="line">    <span class="comment">#torch.save(tudui.state_dict(), "tudui_{}.pth".format(i))</span></span><br><span class="line">    print(<span class="string">"模型已保存"</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></tbody></table></figure><h4 id="test-py"><a href="#test-py" class="headerlink" title="test.py"></a>test.py</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">imgs_name = os.listdir(<span class="string">"my_imgs"</span>)</span><br><span class="line"><span class="comment">#print(imgs_name)</span></span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">"resnet18_catdog_0.9896.pth"</span>, map_location=torch.device(<span class="string">"cpu"</span>))</span><br><span class="line"><span class="comment">#print(model)</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([transforms.Resize(<span class="number">224</span>),</span><br><span class="line">                                transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                transforms.ToTensor(),</span><br><span class="line">                                transforms.Normalize([<span class="number">0.5</span>, ], [<span class="number">0.5</span>, ])])</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(imgs_name)):</span><br><span class="line">    img_name = imgs_name[i]</span><br><span class="line">    img_path = os.path.join(<span class="string">"my_imgs"</span>, img_name)</span><br><span class="line"></span><br><span class="line">    img_PIL = Image.open(img_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 有的图片可能是四通道，所以必须要转换成rgb</span></span><br><span class="line">    img_PIL = img_PIL.convert(<span class="string">"RGB"</span>)</span><br><span class="line">    <span class="comment">#print(img_PIL)</span></span><br><span class="line"></span><br><span class="line">    img = transform(img_PIL)</span><br><span class="line">    <span class="comment">#print(img)</span></span><br><span class="line">    <span class="comment">#print(img.shape)</span></span><br><span class="line"></span><br><span class="line">    img = torch.reshape(img, (<span class="number">-1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="comment">#print(img.shape)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output = model(img)</span><br><span class="line">    <span class="comment">#print(output)</span></span><br><span class="line">    <span class="keyword">if</span> output.argmax(<span class="number">1</span>).item() == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"图片名称为：{}, 识别结果为：cat"</span>.format(img_name))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"图片名称为：{}, 识别结果为：dog"</span>.format(img_name))</span><br></pre></td></tr></tbody></table></figure><h3 id="自动写诗"><a href="#自动写诗" class="headerlink" title="自动写诗"></a>自动写诗</h3><h4 id="model-py-1"><a href="#model-py-1" class="headerlink" title="model,.py"></a>model,.py</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PoetryModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim)</span>:</span></span><br><span class="line">        super(PoetryModel, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=<span class="number">3</span>)</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(self.hidden_dim, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">2048</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">2048</span>, vocab_size)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden=None)</span>:</span></span><br><span class="line">        seq_len, batch_size = input.size()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> hidden <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            h_0 = input.data.new(<span class="number">3</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).float()</span><br><span class="line">            c_0 = input.data.new(<span class="number">3</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).float()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            h_0, c_0 = hidden</span><br><span class="line"></span><br><span class="line">        embeds = self.embedding(input)</span><br><span class="line">        output, hidden = self.lstm(embeds, (h_0, c_0))</span><br><span class="line">        output = self.classifier(output.view(seq_len * batch_size, <span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br></pre></td></tr></tbody></table></figure><h4 id="train-py-2"><a href="#train-py-2" class="headerlink" title="train.py"></a>train.py</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> PoetryModel</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义设备</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入预处理的数据</span></span><br><span class="line">datas = np.load(<span class="string">"./tang.npz"</span>, allow_pickle=<span class="literal">True</span>)</span><br><span class="line">data = datas[<span class="string">'data'</span>]</span><br><span class="line">ix2word = datas[<span class="string">'ix2word'</span>].item()</span><br><span class="line">word2ix = datas[<span class="string">'word2ix'</span>].item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为torch.Tensor</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">data = torch.from_numpy(data)</span><br><span class="line">train_loader = DataLoader(data, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 配置模型，是否继续上一次的训练</span></span><br><span class="line"><span class="comment"># model = PoetryModel(len(word2ix), embedding_dim=128, hidden_dim=256)</span></span><br><span class="line"><span class="comment"># model = model.to(device)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># #加载已训练的模型</span></span><br><span class="line">model = torch.load(<span class="string">"poetry_model_1.5557.pth"</span>)</span><br><span class="line"><span class="comment"># print(model)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment">#优化器</span></span><br><span class="line">learning_rate = <span class="number">5e-5</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"><span class="comment"># optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=5e-4)</span></span><br><span class="line">scheduler = StepLR(optimizer, step_size=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置训练网络的一些参数</span></span><br><span class="line"><span class="comment">#记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment">#记录训练的轮数</span></span><br><span class="line">epoch = <span class="number">300</span>  <span class="comment"># to-do</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">"./log_train"</span>)</span><br><span class="line"></span><br><span class="line">train_losses = []</span><br><span class="line"><span class="comment">#训练步骤开始</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, epoch+<span class="number">1</span>):</span><br><span class="line">    print(<span class="string">"-------------第{}轮训练开始-------------"</span>.format(i))</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        data = data.long().transpose(<span class="number">1</span>, <span class="number">0</span>).contiguous()</span><br><span class="line">        sentences, targets = data[:<span class="number">-1</span>, :], data[<span class="number">1</span>:, :]</span><br><span class="line">        sentences = sentences.to(device)</span><br><span class="line">        targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">        outputs, _ = model(sentences)</span><br><span class="line"></span><br><span class="line">        loss = loss_fn(outputs, targets.view(<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">#优化器优化模型</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        train_loss += loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (batch_idx+<span class="number">1</span>) % <span class="number">200</span> == <span class="number">0</span>:  <span class="comment">#以每个epoch里的batch_idx为点</span></span><br><span class="line">            <span class="comment">#注意  len(train_loader.dataset)</span></span><br><span class="line">            <span class="comment">#     len(train_loader)</span></span><br><span class="line">            <span class="comment">#     len(data[1])</span></span><br><span class="line">            print(<span class="string">'[{}/{} ({:.0f}%)]\tloss: {:.6f}'</span>.format(</span><br><span class="line">                (batch_idx+<span class="number">1</span>) *batch_size, len(train_loader.dataset),</span><br><span class="line">                       <span class="number">100.</span> * (batch_idx+<span class="number">1</span>) / len(train_loader), loss.item()))</span><br><span class="line"></span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    averge_train_loss = train_loss * batch_size / len(train_loader.dataset)</span><br><span class="line">    end_time = time.time()</span><br><span class="line">    print(<span class="string">"-------------第{}轮训练结束-------------"</span>.format(i))</span><br><span class="line">    print(<span class="string">"average loss：{} 距离开始过去时间：{}s"</span>.format(averge_train_loss, end_time - start_time))</span><br><span class="line">    writer.add_scalar(<span class="string">"average_loss"</span>, averge_train_loss, i)</span><br><span class="line">    scheduler.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (averge_train_loss &lt; <span class="number">1.56</span>):</span><br><span class="line">        torch.save(model, <span class="string">"poetry_model_{}.pth"</span>.format(i))</span><br><span class="line">        <span class="comment"># torch.save(tudui.state_dict(), "tudui_{}.pth".format(i))</span></span><br><span class="line">    print(<span class="string">"模型已保存"</span>)</span><br></pre></td></tr></tbody></table></figure><h4 id="test-py-1"><a href="#test-py-1" class="headerlink" title="test.py"></a>test.py</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(model, start_words, ix2word, word2ix, max_gen_len, prefix_words=None)</span>:</span></span><br><span class="line">    <span class="comment"># 读取唐诗的第一句</span></span><br><span class="line">    results = list(start_words)</span><br><span class="line">    start_word_len = len(start_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置第一个词为&lt;START&gt;</span></span><br><span class="line">    input = torch.Tensor([word2ix[<span class="string">'&lt;START&gt;'</span>]]).view(<span class="number">1</span>, <span class="number">1</span>).long()</span><br><span class="line">    input = input.to(device)</span><br><span class="line">    hidden = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> prefix_words:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> prefix_words:</span><br><span class="line">            output, hidden = model(input, hidden)</span><br><span class="line">            input = Variable(input.data.new([word2ix[word]])).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成唐诗</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_gen_len):</span><br><span class="line">        output, hidden = model(input, hidden)</span><br><span class="line">        <span class="comment"># 读取第一句</span></span><br><span class="line">        <span class="keyword">if</span> i &lt; start_word_len:</span><br><span class="line">            w = results[i]</span><br><span class="line">            input = input.data.new([word2ix[w]]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 生成后面的句子</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()</span><br><span class="line">            w = ix2word[top_index]</span><br><span class="line">            results.append(w)</span><br><span class="line">            input = input.data.new([top_index]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 结束标志</span></span><br><span class="line">        <span class="keyword">if</span> w == <span class="string">'&lt;EOP&gt;'</span>:</span><br><span class="line">            <span class="keyword">del</span> results[<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义设备</span></span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">    datas = np.load(<span class="string">"./tang.npz"</span>, allow_pickle=<span class="literal">True</span>)</span><br><span class="line">    data = datas[<span class="string">'data'</span>]</span><br><span class="line">    ix2word = datas[<span class="string">'ix2word'</span>].item()</span><br><span class="line">    word2ix = datas[<span class="string">'word2ix'</span>].item()</span><br><span class="line"></span><br><span class="line">    model = torch.load(<span class="string">"poetry_model_1.5557.pth"</span>)</span><br><span class="line"></span><br><span class="line">    start_words = <span class="string">'瀚海阑干百丈冰'</span>  <span class="comment"># 唐诗的第一句</span></span><br><span class="line">    max_gen_len = <span class="number">64</span>  <span class="comment"># 生成唐诗的最长长度</span></span><br><span class="line"></span><br><span class="line">    prefix_words = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(model)</span></span><br><span class="line">    results = generate(model, start_words, ix2word, word2ix, max_gen_len, prefix_words)</span><br><span class="line">    poetry = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> results:</span><br><span class="line">        poetry += word</span><br><span class="line">        <span class="keyword">if</span> word == <span class="string">'。'</span> <span class="keyword">or</span> word == <span class="string">'!'</span>:</span><br><span class="line">            poetry += <span class="string">'\n'</span></span><br><span class="line"></span><br><span class="line">    print(poetry)</span><br></pre></td></tr></tbody></table></figure><h3 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h3><p>config.py</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">Embedding_size = <span class="number">50</span></span><br><span class="line">Batch_Size = <span class="number">32</span></span><br><span class="line">Epoch = <span class="number">300</span></span><br><span class="line">Learning_rate = <span class="number">1e-4</span></span><br><span class="line">sequence_length = <span class="number">68</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成词频大于20的txt</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_word_freq_txt</span><span class="params">(train_data)</span>:</span></span><br><span class="line">    comments_len = train_data.iloc[:, <span class="number">1</span>].apply(<span class="keyword">lambda</span> x: len(x.split()))</span><br><span class="line">    <span class="comment"># 每一条评论的词数个数。</span></span><br><span class="line">    <span class="comment"># print(comments_len)</span></span><br><span class="line"></span><br><span class="line">    train_data[<span class="string">"comments_len"</span>] = comments_len  <span class="comment"># 每条评论的长度</span></span><br><span class="line">    print(train_data[<span class="string">"comments_len"</span>].describe(percentiles=[<span class="number">.5</span>,<span class="number">.99</span>]))  <span class="comment">#本来是三分位的，这里自己选了两个</span></span><br><span class="line"></span><br><span class="line">    words = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_data)):  <span class="comment"># 遍历每个句子</span></span><br><span class="line">        com = train_data[<span class="string">"comment"</span>][i].split()  <span class="comment"># 把每个句子切分成单词</span></span><br><span class="line">        words = words + com</span><br><span class="line">    <span class="comment">#print(len(words))  # 一个列表，包含重复的单词</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#if os.path.exists("Dataset/word_freq.txt") == None:</span></span><br><span class="line">    <span class="comment"># 挑选出词频大于10的单词放到txt文件中 most_common中可以传参数代表,不带参数表示所有</span></span><br><span class="line">    Freq = <span class="number">30</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"Dataset/word_freq.txt"</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        <span class="keyword">for</span> word, freq <span class="keyword">in</span> Counter(words).most_common():</span><br><span class="line">            <span class="keyword">if</span> freq &gt; Freq:</span><br><span class="line">                fout.write(word + <span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>  train_data</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_vocab</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 初始化vocab</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"Dataset/word_freq.txt"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        vocab = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> fin] <span class="comment">#换行间隔分割用strip</span></span><br><span class="line">    <span class="comment"># print(type(vocab))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造word2idx、idx2word</span></span><br><span class="line">    word2idx = {word: index <span class="keyword">for</span> index, word <span class="keyword">in</span> enumerate(vocab)}</span><br><span class="line">    idx2word = {index: word <span class="keyword">for</span> index, word <span class="keyword">in</span> enumerate(vocab)}  <span class="comment"># 没有想到列表竟然可以枚举。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#print(vocab)  # 词典</span></span><br><span class="line">    <span class="comment"># print(len(vocab))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># idx = word2idx["一直"]</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># str1 = idx2word[104]</span></span><br><span class="line">    <span class="comment"># print(str1)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab, word2idx, idx2word</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#对输入数据进行预处理,主要是对句子用索引表示且对句子进行截断与padding，将填充使用”把“来。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span><span class="params">(train_data, sequence_length, word2idx)</span>:</span></span><br><span class="line">    inputs = []</span><br><span class="line">    sentence_char = [i.split() <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="string">"comment"</span>]]  <span class="comment">#空格间隔分割用split</span></span><br><span class="line">    <span class="comment">#print(sentence_char)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将输入文本进行padding</span></span><br><span class="line">    <span class="keyword">for</span> index, i <span class="keyword">in</span> enumerate(sentence_char):</span><br><span class="line">        temp=[word2idx.get(j,<span class="number">46</span>) <span class="keyword">for</span> j <span class="keyword">in</span> i] <span class="comment">#表示如果词表中没有这个稀有词，无法获得，那么就默认返回pad_id。</span></span><br><span class="line">        <span class="comment">#print(temp)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(len(i)&lt;sequence_length):</span><br><span class="line">            <span class="comment">#应该padding。</span></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(sequence_length-len(i)):</span><br><span class="line">                temp.append(<span class="number">46</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            temp = temp[:sequence_length]</span><br><span class="line">        inputs.append(temp)</span><br><span class="line">    <span class="keyword">return</span> inputs <span class="comment">#二维列表</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNNDataSet</span><span class="params">(Data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_inputs, data_targets)</span>:</span></span><br><span class="line">        self.inputs = torch.LongTensor(data_inputs)</span><br><span class="line">        self.label = torch.LongTensor(data_targets)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.inputs[index], self.label[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.inputs)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word2vec</span><span class="params">(x,w2v,idx2word)</span>:</span></span><br><span class="line">    <span class="comment">#x:batch_size,sequence_length</span></span><br><span class="line">    <span class="comment">#-》x:batch_size,sequence_length,embedding_size</span></span><br><span class="line">    <span class="comment">#x是以编号的形式来反映的，所以需要将其翻译一下。</span></span><br><span class="line">    x2v=np.ones((len(x),x.shape[<span class="number">1</span>],Embedding_size))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line"><span class="comment">#         seqtext=[idx2char[j.item()] for j in x[i]]</span></span><br><span class="line">        x2v[i]=w2v[[idx2word[j.item()] <span class="keyword">for</span> j <span class="keyword">in</span> x[i]]]</span><br><span class="line">        <span class="comment">#print(x2v[i].shape)</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor(x2v).to(torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># f2 = open("Dataset/validation.txt", 'r', encoding='utf-8')</span></span><br><span class="line">    <span class="comment"># f3 = open("Dataset/sum.txt", 'a+', encoding='utf-8')</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># for i in f2:</span></span><br><span class="line">    <span class="comment">#     f3.write(i)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># f2.close()</span></span><br><span class="line">    <span class="comment"># f3.close()</span></span><br><span class="line"></span><br><span class="line">    train_data = pd.read_csv(<span class="string">"Dataset/sum.txt"</span>, names=[<span class="string">"label"</span>, <span class="string">"comment"</span>], sep=<span class="string">"\t"</span>)</span><br><span class="line">    generate_word_freq_txt(train_data)</span><br></pre></td></tr></tbody></table></figure><p>model.py</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> Embedding_size, sequence_length</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(  <span class="comment"># batch_size, 1,sequence_length, embedding_size</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, (<span class="number">2</span>, Embedding_size)),  <span class="comment"># 卷积核大小为2*Embedding_size,默认当然是步长为1    # batch_size,10,seq_len-1,1</span></span><br><span class="line">            nn.ReLU(),  <span class="comment"># batch_size,10,seq_len-1,1</span></span><br><span class="line">            nn.MaxPool2d((sequence_length - <span class="number">1</span>, <span class="number">1</span>)),  <span class="comment"># batch_size,10,1,1########直接被maxpooliing了，从一个序列变成一个向量，表示将整个句子选出一个最关键的情感分类词来。</span></span><br><span class="line">            nn.Flatten(),   <span class="comment"># [batch_size, 10]</span></span><br><span class="line">            nn.Dropout(<span class="number">0.8</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">10</span>, <span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    model = Model()</span><br><span class="line">    input = torch.ones(<span class="number">64</span>, <span class="number">1</span>, <span class="number">68</span>, <span class="number">50</span>)</span><br><span class="line">    output = model(input)</span><br><span class="line">    print(output.shape)</span><br></pre></td></tr></tbody></table></figure><p>train.py</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> keyedvectors</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> TextCNNDataSet, generate_vocab, tokenizer, sequence_length, Learning_rate, Epoch, Batch_Size, \</span><br><span class="line">    word2vec</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    np.random.seed(<span class="number">0</span>)</span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">    torch.cuda.manual_seed_all(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># torch.backends.cudnn.benchmark = False</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义设备</span></span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line"></span><br><span class="line">    train_data = pd.read_csv(<span class="string">"Dataset/train.txt"</span>, names=[<span class="string">"label"</span>, <span class="string">"comment"</span>], sep=<span class="string">"\t"</span>)</span><br><span class="line">    valid_data = pd.read_csv(<span class="string">"Dataset/validation.txt"</span>, names=[<span class="string">"label"</span>, <span class="string">"comment"</span>], sep=<span class="string">"\t"</span>)</span><br><span class="line">    train_len  = len(train_data)</span><br><span class="line">    <span class="comment">#print(train_len)</span></span><br><span class="line">    valid_len = len(valid_data)</span><br><span class="line">    <span class="comment"># print(len(train_data)) #多少条句子</span></span><br><span class="line">    <span class="comment"># print(train_data["comment"])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate_word_freq_txt(train_data)</span></span><br><span class="line">    vocab, word2idx, idx2word = generate_vocab()</span><br><span class="line">    <span class="comment"># #print(vocab)</span></span><br><span class="line">    <span class="comment"># # idx = word2idx["把"]</span></span><br><span class="line">    <span class="comment"># # print(idx)</span></span><br><span class="line"></span><br><span class="line">    train_input = tokenizer(train_data, sequence_length, word2idx)</span><br><span class="line">    valid_input = tokenizer(valid_data, sequence_length, word2idx)</span><br><span class="line"></span><br><span class="line">    train_set = TextCNNDataSet(train_input,  list(train_data[<span class="string">"label"</span>]))</span><br><span class="line">    valid_set = TextCNNDataSet(valid_input, list(valid_data[<span class="string">"label"</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test_size = int(len(data_input) * 0.05)</span></span><br><span class="line">    <span class="comment"># train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(TextCNNDataSet,[train_size, val_size, test_size])</span></span><br><span class="line"></span><br><span class="line">    train_loader = DataLoader(train_set, batch_size=Batch_Size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    valid_loader = DataLoader(valid_set, batch_size=Batch_Size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># TestDataLoader = Data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)</span></span><br><span class="line">    <span class="comment"># print(train_size)</span></span><br><span class="line"></span><br><span class="line">    w2v = keyedvectors.load_word2vec_format(<span class="string">"Dataset/wiki_word2vec_50.bin"</span>, binary=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#print(w2v[["的", "在"]])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于补充无法转为词向量的词，随机分配。不然会报错。</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(vocab)):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            w2v[vocab[i]] = w2v[vocab[i]]</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            w2v[vocab[i]] = np.random.randn(<span class="number">50</span>, )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 搭建模型</span></span><br><span class="line">    <span class="comment"># model = Model()</span></span><br><span class="line">    <span class="comment"># model = model.to(device)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#加载已训练的模型</span></span><br><span class="line">    model = torch.load(<span class="string">"ec_model_0.8148.pth"</span>)      <span class="comment">#to-do</span></span><br><span class="line">    <span class="comment">#print(model)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">    loss_fn = loss_fn.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 优化器</span></span><br><span class="line">    <span class="comment"># learning_rate = 0.01</span></span><br><span class="line">    <span class="comment"># 1e-2 = 1*(10)^(-2)</span></span><br><span class="line">    <span class="comment">#learning_rate = 1e-3  # to-do</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=Learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置训练网络的一些参数</span></span><br><span class="line">    <span class="comment"># 记录训练的次数</span></span><br><span class="line">    total_train_step = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 记录测试的次数</span></span><br><span class="line">    total_valid_step = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 记录训练的轮数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加tensorboard</span></span><br><span class="line">    writer = SummaryWriter(<span class="string">"log_train"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集每batch_size保存(batch_size的train_loss)，并输出到tensorboard</span></span><br><span class="line">    <span class="comment"># 测试集每一个epoch(也就是把测试集都过一遍)保存(总体的test_loss)、(total_test_accuracy)，并输出到tensorboard</span></span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Epoch):</span><br><span class="line">        print(<span class="string">"-------------第{}轮训练开始-------------"</span>.format(i + <span class="number">1</span>))</span><br><span class="line">        total_train_true_num = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 训练步骤开始</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">            inputs, targets = data</span><br><span class="line"></span><br><span class="line">            <span class="comment"># print(imgs.shape)</span></span><br><span class="line">            targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">            vec_inputs = word2vec(inputs,w2v,idx2word).unsqueeze(<span class="number">1</span>)</span><br><span class="line">            vec_inputs = vec_inputs.to(device)</span><br><span class="line">            <span class="comment">#print(vec_inputs.shape)</span></span><br><span class="line">            outputs = model(vec_inputs)</span><br><span class="line">            <span class="comment"># print(outputs.shape)</span></span><br><span class="line"></span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">            train_true_num = (outputs.argmax(<span class="number">1</span>) == targets).sum()</span><br><span class="line">            total_train_true_num += train_true_num</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 优化器优化模型</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                end_time = time.time()</span><br><span class="line">                print(end_time - start_time)</span><br><span class="line">                print(<span class="string">"训练次数：{}, loss：{}"</span>.format(total_train_step, loss.item()))</span><br><span class="line">                writer.add_scalar(<span class="string">"train_loss"</span>, loss.item(), total_train_step)</span><br><span class="line">            total_train_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        total_train_accuracy = total_train_true_num / train_len</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 测试步骤开始</span></span><br><span class="line">        model.eval()</span><br><span class="line">        total_valid_loss = <span class="number">0</span></span><br><span class="line">        total_valid_true_num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> data <span class="keyword">in</span> valid_loader:</span><br><span class="line">                inputs, targets = data</span><br><span class="line"></span><br><span class="line">                targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">                vec_inputs = word2vec(inputs, w2v, idx2word).unsqueeze(<span class="number">1</span>)</span><br><span class="line">                vec_inputs = vec_inputs.to(device)</span><br><span class="line">                <span class="comment"># print(vec_inputs.shape)</span></span><br><span class="line">                outputs = model(vec_inputs)</span><br><span class="line"></span><br><span class="line">                loss = loss_fn(outputs, targets)</span><br><span class="line">                total_valid_loss += loss.item()</span><br><span class="line">                valid_true_num = (outputs.argmax(<span class="number">1</span>) == targets).sum()</span><br><span class="line">                total_valid_true_num += valid_true_num</span><br><span class="line"></span><br><span class="line">        total_valid_accuracy = total_valid_true_num / valid_len</span><br><span class="line">        print(<span class="string">"整个训练集上的正确率：{}，整个验证集上的loss：{}，整个验证集上的正确率：{}"</span>.format(total_train_accuracy, total_valid_loss,</span><br><span class="line">                                                                  total_valid_accuracy))</span><br><span class="line">        writer.add_scalar(<span class="string">"test_loss"</span>, total_valid_loss, total_valid_step)</span><br><span class="line">        writer.add_scalar(<span class="string">"test_accuracy"</span>, total_valid_accuracy, total_valid_step)</span><br><span class="line">        total_valid_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (total_train_accuracy &gt; <span class="number">0.90</span> <span class="keyword">and</span> total_valid_accuracy &gt; <span class="number">0.81</span>):</span><br><span class="line">            torch.save(model, <span class="string">"ec_model_{}.pth"</span>.format(i))</span><br><span class="line">        <span class="comment"># torch.save(tudui.state_dict(), "tudui_{}.pth".format(i))</span></span><br><span class="line">        print(<span class="string">"模型已保存"</span>)</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br></pre></td></tr></tbody></table></figure><p>test.py</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> keyedvectors</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> TextCNNDataSet, generate_vocab, tokenizer, sequence_length, Learning_rate, Epoch, Batch_Size, \</span><br><span class="line">    word2vec</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,precision_score,recall_score,f1_score</span><br><span class="line"></span><br><span class="line">test_data = pd.read_csv(<span class="string">"Dataset/train.txt"</span>, names=[<span class="string">"label"</span>, <span class="string">"comment"</span>], sep=<span class="string">"\t"</span>)</span><br><span class="line">vocab, word2idx, idx2word = generate_vocab()</span><br><span class="line">test_input = tokenizer(test_data, sequence_length, word2idx)</span><br><span class="line">test_set = TextCNNDataSet(test_input,  list(test_data[<span class="string">"label"</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#print(type(test_set.label))</span></span><br><span class="line"></span><br><span class="line">w2v = keyedvectors.load_word2vec_format(<span class="string">"Dataset/wiki_word2vec_50.bin"</span>, binary=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#print(w2v[["的", "在"]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于补充无法转为词向量的词，随机分配。不然会报错。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(vocab)):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        w2v[vocab[i]] = w2v[vocab[i]]</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        w2v[vocab[i]] = np.random.randn(<span class="number">50</span>, )</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载已训练的模型</span></span><br><span class="line">model = torch.load(<span class="string">"ec_model_0.8122.pth"</span>, map_location=torch.device(<span class="string">"cpu"</span>))      <span class="comment">#to-do</span></span><br><span class="line"></span><br><span class="line">vec_inputs = word2vec(test_set.inputs,w2v,idx2word).unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y_predict = model(vec_inputs)</span><br><span class="line">y_predict = y_predict.argmax(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y = test_set.label.detach().numpy()</span><br><span class="line"></span><br><span class="line">y_predict  = y_predict.detach().numpy()</span><br><span class="line"></span><br><span class="line">confusion_matrix = confusion_matrix(y,y_predict) <span class="comment">#混淆矩阵</span></span><br><span class="line">precision_score = precision_score(y,y_predict)  <span class="comment">#准确率</span></span><br><span class="line">recall_score = recall_score(y,y_predict) <span class="comment">#召回率</span></span><br><span class="line">f1_score = f1_score(y,y_predict)  <span class="comment"># f1</span></span><br><span class="line">print(<span class="string">"准确率：{}\n召回率：{}\nf1：{}\n混淆矩阵：\n{}"</span>.format(precision_score,recall_score,f1_score,confusion_matrix))</span><br></pre></td></tr></tbody></table></figure><script>document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });</script></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>觉得文章对您有帮助，请我喝杯咖啡吧^_^</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="data:image/png;base64,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" data-original="/images/wechatpay.jpg" alt="春风化雨 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="data:image/png;base64,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" data-original="/images/alipay.jpg" alt="春风化雨 支付宝"><p>支付宝</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a><a href="/tags/pytorch/" rel="tag"><i class="fa fa-tag"></i> pytorch</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/category/%E9%87%91%E5%88%9A%E5%8A%9F/" rel="next" title="金刚功"><i class="fa fa-chevron-left"></i> 金刚功</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/category/%E8%B7%91%E5%AE%8C%E6%AD%A5%E6%8B%89%E4%BC%B8%E5%8A%A8%E4%BD%9C/" rel="prev" title="跑完步拉伸动作">跑完步拉伸动作<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/zuozhu.jpg" alt="春风化雨"><p class="site-author-name" itemprop="name">春风化雨</p><p class="site-description motion-element" itemprop="description">因为你不会，所以你才会</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">165</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">35</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">43</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/charryglomc" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a></span><span class="links-of-author-item"><a href="mailto:798577670@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div><p></p><div class="hitokoto-title"><i class="fa fa-paragraph"></i> <b>一言</b></div><div id="hitokoto">:D 获取中...</div><i id="hitofrom">:D 获取中...</i><script src="https://cdn.jsdelivr.net/npm/bluebird@3/js/browser/bluebird.min.js"></script><script src="https://cdn.jsdelivr.net/npm/whatwg-fetch@2.0.3/fetch.min.js"></script><script>fetch("https://v1.hitokoto.cn").then(function(t){return t.json()}).then(function(t){document.getElementById("hitokoto").innerText="       "+t.hitokoto,document.getElementById("hitofrom").innerText="             ——"+t.from+" "}).catch(function(t){console.error(t)})</script></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#mnist手写数字识别"><span class="nav-text">mnist手写数字识别</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#model-py"><span class="nav-text">model.py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#train-py"><span class="nav-text">train.py</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#猫狗分类"><span class="nav-text">猫狗分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#get-data-py"><span class="nav-text">get_data.py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#train-py-1"><span class="nav-text">train.py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#test-py"><span class="nav-text">test.py</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自动写诗"><span class="nav-text">自动写诗</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#model-py-1"><span class="nav-text">model,.py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#train-py-2"><span class="nav-text">train.py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#test-py-1"><span class="nav-text">test.py</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#情感分类"><span class="nav-text">情感分类</span></a></li></ol></div></div></section><script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script><script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script><div class="widget-wrap"><h3 class="widget-title">Tag Cloud</h3><div id="myCanvasContainer" class="widget tagcloud"><canvas width="250" height="250" id="resCanvas"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/android/" rel="tag">android</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/background/" rel="tag">background</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/" rel="tag">c</a><span class="tag-list-count">96</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/" rel="tag">c++</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cpp/" rel="tag">cpp</a><span class="tag-list-count">19</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/css/" rel="tag">css</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cv/" rel="tag">cv</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/emmet/" rel="tag">emmet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gdb/" rel="tag">gdb</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gitee/" rel="tag">gitee</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/html/" rel="tag">html</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a><span class="tag-list-count">67</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markdown/" rel="tag">markdown</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/next/" rel="tag">next</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pat/" rel="tag">pat</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rails/" rel="tag">rails</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ruby/" rel="tag">ruby</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ssh/" rel="tag">ssh</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/struct/" rel="tag">struct</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/table/" rel="tag">table</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vscode/" rel="tag">vscode</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/web/" rel="tag">web</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B8%AD%E5%8C%BB/" rel="tag">中医</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%88%97%E8%A1%A8%E5%92%8C%E8%A1%A8%E5%8D%95/" rel="tag">列表和表单</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%9A%E5%AE%A2/" rel="tag">博客</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E6%BA%AF/" rel="tag">回溯</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%97%E4%BD%93/" rel="tag">字体</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%87%E7%AD%BE/" rel="tag">标签</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%87%E7%AD%BE%E6%98%BE%E7%A4%BA%E6%A8%A1%E5%BC%8F/" rel="tag">标签显示模式</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%B7%E5%BC%8F%E8%A1%A8/" rel="tag">样式表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">51</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AF%AE%E7%90%83/" rel="tag">篮球</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/" rel="tag">编译原理</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%80%83%E7%A0%94/" rel="tag">考研</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%90%E5%8A%A8/" rel="tag">运动</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E5%99%A8/" rel="tag">选择器</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%87%91%E5%88%9A%E5%8A%9F/" rel="tag">金刚功</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E7%AC%94/" rel="tag">随笔</a><span class="tag-list-count">1</span></li></ul></canvas></div></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_pv">总访问量:<span id="busuanzi_value_site_pv"></span>次</span> <span class="post-meta-divider">|</span> <span id="busuanzi_container_site_uv">总访客:<span id="busuanzi_value_site_uv"></span>人</span> <span class="post-meta-divider">|</span> <span class="post-count">全站共 179.8k 字</span><div class="copyright">&copy; <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">春风化雨</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><style>.copy-btn{display:inline-block;padding:6px 12px;font-size:13px;font-weight:700;line-height:20px;color:#333;white-space:nowrap;vertical-align:middle;cursor:pointer;background-color:#eee;background-image:linear-gradient(#fcfcfc,#eee);border:1px solid #d5d5d5;border-radius:3px;user-select:none;outline:0}.highlight-wrap .copy-btn{transition:opacity .3s ease-in-out;opacity:0;padding:2px 6px;position:absolute;right:4px;top:8px}.highlight-wrap .copy-btn:focus,.highlight-wrap:hover .copy-btn{opacity:1}.highlight-wrap{position:relative}</style><script>$(".highlight").each(function(t,e){var n=$("<div>").addClass("highlight-wrap");$(e).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(t){var e=$(this).parent().find(".code").find(".line").map(function(t,e){return $(e).text()}).toArray().join("\n"),n=document.createElement("textarea");document.body.appendChild(n),n.style.position="absolute",n.style.top="0px",n.style.left="0px",n.value=e,n.select(),n.focus();var o=document.execCommand("copy");document.body.removeChild(n),o?$(this).text("复制成功"):$(this).text("复制失败"),$(this).blur()})).on("mouseleave",function(t){var e=$(this).find(".copy-btn");setTimeout(function(){e.text("复制")},300)}).append(e)})</script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script type="text/javascript">var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'lMeCamY9oc4xlFhotJVSd1sX-gzGzoHsz',
        appKey: 'GQGSN3lv8dwU6TWn3fJrU65z',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,s){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var o=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,n=document.getElementById(e),r=document.getElementById(s);n.addEventListener("input",function(){var y=n.value.trim().toLowerCase(),T=y.split(/[\s\-]+/);1<T.length&&T.push(y);var b=[];if(0<y.length&&o.forEach(function(t){function e(t,e,o,n){for(var r=n[n.length-1],s=r.position,a=r.word,i=[],c=0;s+a.length<=o&&0!=n.length;){a===y&&c++,i.push({position:s,length:a.length});var l=s+a.length;for(n.pop();0!=n.length&&(s=(r=n[n.length-1]).position,a=r.word,s<l);)n.pop()}return h+=c,{hits:i,start:e,end:o,searchTextCount:c}}function o(o,t){var n="",r=t.start;return t.hits.forEach(function(t){n+=o.substring(r,t.position);var e=t.position+t.length;n+='<b class="search-keyword">'+o.substring(t.position,e)+"</b>",r=e}),n+=o.substring(r,t.end)}var n=!1,r=0,h=0,s=t.title.trim(),a=s.toLowerCase(),i=t.content.trim().replace(/<[^>]+>/g,""),c=i.toLowerCase(),l=decodeURIComponent(t.url),p=[],u=[];if(""!=s&&(T.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());-1<(s=e.indexOf(t,r));)a.push({position:s,word:t}),r=s+n;return a}p=p.concat(e(t,a,!1)),u=u.concat(e(t,c,!1))}),(0<p.length||0<u.length)&&(n=!0,r=p.length+u.length)),n){[p,u].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var f=[];0!=p.length&&f.push(e(0,0,s.length,p));for(var d=[];0!=u.length;){var g=u[u.length-1],v=g.position,$=g.word,C=v-20,m=v+80;C<0&&(C=0),m<v+$.length&&(m=v+$.length),m>i.length&&(m=i.length),d.push(e(0,C,m,u))}d.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var x=parseInt("1");0<=x&&(d=d.slice(0,x));var w="";w+=0!=f.length?"<li><a href='"+l+"' class='search-result-title'>"+o(s,f[0])+"</a>":"<li><a href='"+l+"' class='search-result-title'>"+s+"</a>",d.forEach(function(t){w+="<a href='"+l+'\'><p class="search-result">'+o(i,t)+"...</p></a>"}),w+="</li>",b.push({item:w,searchTextCount:h,hitCount:r,id:b.length})}}),1===T.length&&""===T[0])r.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===b.length)r.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{b.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var e='<ul class="search-result-list">';b.forEach(function(t){e+=t.item}),e+="</ul>",r.innerHTML=e}}),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),!1===isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){27===t.which&&$(".search-popup").is(":visible")&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("lMeCamY9oc4xlFhotJVSd1sX-gzGzoHsz","GQGSN3lv8dwU6TWn3fJrU65z")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),t.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.copy-btn{display:inline-block;padding:6px 12px;font-size:13px;font-weight:700;line-height:20px;color:#333;white-space:nowrap;vertical-align:middle;cursor:pointer;background-color:#eee;background-image:linear-gradient(#fcfcfc,#eee);border:1px solid #d5d5d5;border-radius:3px;user-select:none;outline:0}.highlight-wrap .copy-btn{transition:opacity .3s ease-in-out;opacity:0;padding:2px 6px;position:absolute;right:4px;top:8px}.highlight-wrap .copy-btn:focus,.highlight-wrap:hover .copy-btn{opacity:1}.highlight-wrap{position:relative}</style><script>$(".highlight").each(function(t,e){var n=$("<div>").addClass("highlight-wrap");$(e).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(t){var e=$(this).parent().find(".code").find(".line").map(function(t,e){return $(e).text()}).toArray().join("\n"),n=document.createElement("textarea");document.body.appendChild(n),n.style.position="absolute",n.style.top="0px",n.style.left="0px",n.value=e,n.select(),n.focus();var o=document.execCommand("copy");document.body.removeChild(n),o?$(this).text("复制成功"):$(this).text("复制失败"),$(this).blur()})).on("mouseleave",function(t){var e=$(this).find(".copy-btn");setTimeout(function(){e.text("复制")},300)}).append(e)})</script><script type="text/javascript" src="/js/src/sakura.js"></script><script>!function(e){function i(){for(var r=0;r<c.length;r++)0<=(t=c[r].getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n()},e.src=i}();var t}var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this)</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})})</script><script type="text/x-mathjax-config">MathJax.Hub.Config("");</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src=""></script></body></html>